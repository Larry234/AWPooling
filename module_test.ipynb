{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9076a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "path = 'HPO/tiny-imagenet/scratch/vgg13aw'\n",
    "path = glob(os.path.join(path, '*'))\n",
    "\n",
    "tems = []\n",
    "accs = []\n",
    "\n",
    "for p in path:\n",
    "    if not os.path.isdir(p) or 'error.pkl' in os.listdir(p):\n",
    "        continue\n",
    "    trial_name = \"\"\n",
    "    csv_f = pd.read_csv(os.path.join(p, 'progress.csv'))\n",
    "    accs.append(max(csv_f['accuracy']))\n",
    "    f = open(os.path.join(p, 'params.json'))\n",
    "    json_f = json.load(f)\n",
    "    \n",
    "    params = list(json_f.keys())[-5:]\n",
    "    values = list(json_f.values())[-5:]\n",
    "    for t, v in zip(params, values):\n",
    "        trial_name += f'{t}={v},'\n",
    "    tems.append(trial_name)\n",
    "        \n",
    "df = pd.DataFrame({'tems': tems, 'accs': accs})\n",
    "df = df.sort_values(by=['accs'], ascending=False)\n",
    "path = 'HPO/tiny-imagenet/scratch/vgg13aw'\n",
    "df.to_csv(os.path.join(path, 'best.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5e4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [(0.561199963092804, 's'), (0.5543999671936035, 'a'), (0.5496000051498413, 's'), (0.5496000051498413, 'a')], 'pool': 'assa'}\n",
      "{'acc': [(0.5601999759674072, 'a'), (0.5511999726295471, 's'), (0.5496000051498413, 's'), (0.5425999760627747, 's')], 'pool': 'asss'}\n",
      "{'acc': [(0.5601999759674072, 's'), (0.5573999881744385, 'a'), (0.5532000064849854, 's'), (0.5529999732971191, 'a'), (0.5509999990463257, 's'), (0.5501999855041504, 's'), (0.5501999855041504, 'a'), (0.5478000044822693, 's'), (0.5471999645233154, 's'), (0.5453999638557434, 's'), (0.5447999835014343, 's')], 'pool': 'aaaa'}\n",
      "{'acc': [(0.5582000017166138, 's'), (0.5569999814033508, 's'), (0.5507999658584595, 's'), (0.5491999983787537, 'a'), (0.5478000044822693, 's'), (0.54339998960495, 's'), (0.5356000065803528, 's')], 'pool': 'aass'}\n",
      "{'acc': [(0.5561999678611755, 'a'), (0.5559999942779541, 'a'), (0.5546000003814697, 'a'), (0.5507999658584595, 'a'), (0.5507999658584595, 'a'), (0.550599992275238, 's'), (0.5489999651908875, 's'), (0.5478000044822693, 'a'), (0.546999990940094, 's'), (0.5465999841690063, 'a'), (0.5446000099182129, 'a')], 'pool': 'ssaa'}\n",
      "{'acc': [(0.5557999610900879, 'a'), (0.553600013256073, 'a'), (0.5523999929428101, 'a'), (0.5509999990463257, 's'), (0.5483999848365784, 'a'), (0.5479999780654907, 'a'), (0.5461999773979187, 'a'), (0.5460000038146973, 's')], 'pool': 'aasa'}\n",
      "{'acc': [(0.5557999610900879, 'a'), (0.5555999875068665, 'a'), (0.550599992275238, 's'), (0.5491999983787537, 'a'), (0.5489999651908875, 'a'), (0.548799991607666, 'a'), (0.5485999584197998, 'a'), (0.5485999584197998, 's'), (0.5467999577522278, 'a')], 'pool': 'saas'}\n",
      "{'acc': [(0.5557999610900879, 'a'), (0.5533999800682068, 'a'), (0.5511999726295471, 'a'), (0.5511999726295471, 's'), (0.5483999848365784, 's'), (0.5473999977111816, 'a'), (0.5461999773979187, 'a'), (0.54339998960495, 'a')], 'pool': 'saaa'}\n",
      "{'acc': [(0.5547999739646912, 'a'), (0.5514000058174133, 'a'), (0.5511999726295471, 'a'), (0.550000011920929, 's'), (0.5497999787330627, 's'), (0.5496000051498413, 's'), (0.5491999983787537, 's'), (0.546999990940094, 'a'), (0.5460000038146973, 's'), (0.5442000031471252, 's')], 'pool': 'asaa'}\n",
      "{'acc': [(0.5547999739646912, 'a'), (0.5519999861717224, 'a'), (0.5511999726295471, 's'), (0.5467999577522278, 's'), (0.5357999801635742, 's')], 'pool': 'ssas'}\n",
      "{'acc': [(0.5546000003814697, 's')], 'pool': 'mssa'}\n",
      "{'acc': [(0.5527999997138977, 'a'), (0.545199990272522, 'a'), (0.5447999835014343, 's')], 'pool': 'sass'}\n",
      "{'acc': [(0.5525999665260315, 'a'), (0.5519999861717224, 's')], 'pool': 'sssa'}\n",
      "{'acc': [(0.5521999597549438, 'a'), (0.5519999861717224, 'a'), (0.5497999787330627, 'a'), (0.5496000051498413, 'a'), (0.5493999719619751, 'a'), (0.5485999584197998, 'a'), (0.5437999963760376, 'a'), (0.5414000153541565, 's'), (0.5399999618530273, 's')], 'pool': 'ssss'}\n",
      "{'acc': [(0.550000011920929, 's'), (0.5473999977111816, 's'), (0.5447999835014343, 's')], 'pool': 'aaas'}\n",
      "{'acc': [(0.5497999787330627, 'a')], 'pool': 'asas'}\n",
      "{'acc': [(0.5478000044822693, 'a'), (0.545199990272522, 'a'), (0.5414000153541565, 'a')], 'pool': 'sasa'}\n",
      "{'acc': [(0.5429999828338623, 'a')], 'pool': 'ssms'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "csv_path = glob('HPO/tiny-imagenet/scratch/vgg19aw/*.csv')\n",
    "save_path = 'HPO/performance'\n",
    "analysis = []\n",
    "\n",
    "for path in csv_path: # read expermiment record\n",
    "    model_name = path.split('/')[-2]\n",
    "    csv = pd.read_csv(path)\n",
    "    \n",
    "    for index, row in csv.iterrows():\n",
    "        acc = row['accs']\n",
    "        tmp = {'acc': acc, 'pool': []}\n",
    "        for tem in row['tems'].split(','):\n",
    "            if tem == '':\n",
    "                break\n",
    "            \n",
    "            name, t = tem.split('=')\n",
    "            t = float(t)\n",
    "            \n",
    "            if t <= 0.1:\n",
    "                tmp['pool'].append('max')\n",
    "            elif t >= 5:\n",
    "                tmp['pool'].append('avg')\n",
    "            else:\n",
    "                tmp['pool'].append('sw')\n",
    "        analysis.append(tmp)\n",
    "        \n",
    "\n",
    "    \n",
    "#     # plot result\n",
    "#     for k, v in analysis.items():\n",
    "        \n",
    "#         plt.title(f'Model performace {k}')\n",
    "#         plt.xticks(np.arange(0, 11, 1))\n",
    "#         plt.xlabel(\"Temperature\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#         plt.scatter(*zip(*v['max']), c='g', marker=6, label='max')\n",
    "#         plt.scatter(*zip(*v['avg']), c='b', marker=6, label='avg')\n",
    "#         plt.scatter(*zip(*v['sw']), c='r', marker=6, label='sw')\n",
    "#         plt.legend(loc=1)\n",
    "#         plt.savefig(os.path.join(save_path, f'{model_name}_{k}.png'))\n",
    "#         plt.clf()\n",
    "                  \n",
    "# csv = pd.read_csv(csv_path[0])\n",
    "# for index, row in csv.iterrows():\n",
    "#     tem = row['tems'].split(',')\n",
    "#     t0 = float(tem[0][3:])\n",
    "#     t4 = float(tem[4][3:])\n",
    "#     acc = row['accs']\n",
    "#     if t4 < 1:\n",
    "#         analysis['max'].append((t4, acc))\n",
    "#     elif t4 > 5:\n",
    "#         analysis['avg'].append((t4, acc))\n",
    "#     else:\n",
    "#         analysis['sw'].append((t4, acc))\n",
    "        \n",
    "# plt.title(f'Independent analysis {t}')\n",
    "# plt.xticks(np.arange(0, 11, 1))\n",
    "# plt.scatter(*zip(*analysis['max']), c='g')\n",
    "# plt.scatter(*zip(*analysis['avg']), c='b')\n",
    "# plt.scatter(*zip(*analysis['sw']), c='r')\n",
    "# plt.savefig(os.path.join(save_path, f'{t}_acc.png'))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32ff4f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6b66a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 200]' is invalid for input of size 6400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 200]' is invalid for input of size 6400"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0e62e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.awpooling import *\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.vggaw import VGG11AW, VGG11AWT\n",
    "import os\n",
    "\n",
    "h = 1e-4\n",
    "writer = SummaryWriter(log_dir=os.path.join('Gradient_analysis', f'delta={h}'))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "ds = ImageFolder(root='/home/larry/Datasets/tiny-imagenet-200/train', transform=transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]))\n",
    "loader = torch.utils.data.DataLoader(ds, shuffle=True, batch_size=128, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3ec1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_class=200):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.aw1 = AWPool2d_()\n",
    "\n",
    "        # self.conv2 = nn.Sequential(\n",
    "        #     nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        # )\n",
    "        # self.aw2 = AWPool2d_()\n",
    "    \n",
    "        self.globalavg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, num_class),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.aw1(x)\n",
    "        x = self.globalavg(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def set_t(self, t):\n",
    "        self.aw1.t = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91e97fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (aw1): AWPool2d_()\n",
       "  (globalavg): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "params = [{'params': p, 'lr': 0.1} for n, p in model.named_parameters() if 'aw' not in n]\n",
    "optimizer = torch.optim.SGD(params)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e81d5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(loader):\n",
    "    model.aw1.t.requires_grad = True\n",
    "    images, label = data\n",
    "    images = images.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    # Autograd\n",
    "    logits = model(images)\n",
    "    base_loss = criterion(logits, label)\n",
    "    base_loss.backward()\n",
    "    writer.add_scalar('Automatic gardient', model.aw1.t.grad, i)\n",
    "\n",
    "    # caculate numercial difference\n",
    "    with torch.no_grad():\n",
    "        model.aw1.t.requires_grad = False\n",
    "        origin_t = model.aw1.t.item()\n",
    "\n",
    "        # forward\n",
    "        model.aw1.t.copy_(torch.tensor(origin_t + h))\n",
    "        logits = model(images)\n",
    "        forward_loss = criterion(logits, label)\n",
    "\n",
    "        # backward\n",
    "        model.aw1.t.copy_(torch.tensor(origin_t - h))\n",
    "        logits = model(images)\n",
    "        backward_loss = criterion(logits, label)\n",
    "\n",
    "        writer.add_scalar('Forward difference', (forward_loss - base_loss) / h, i)\n",
    "        writer.add_scalar('Backward difference', (base_loss - backward_loss) / h, i)\n",
    "        writer.add_scalar('Central difference', (forward_loss - backward_loss) / (2 * h), i)\n",
    "\n",
    "    model.aw1.t.copy_(torch.tensor(origin_t))\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    model.aw1.t.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e1d41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "025f342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-56a11fc98e5285b0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-56a11fc98e5285b0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir Gradient_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adbe27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0037], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = model(x)\n",
    "loss = criterion(logits, y)\n",
    "loss.backward()\n",
    "print(model.aw1.t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597584c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.aw1.t.requires_grad = False\n",
    "logits = model(x)\n",
    "base = criterion(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c644c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0019999742507935\n",
      "0.9980000257492065\n",
      "base loss: 5.295560836791992\n",
      "forward loss: 5.295570373535156\n",
      "backward loss: 5.29556131362915\n",
      "forward difference: 0.004768371116369963\n",
      "backward difference: -0.00023841856454964727\n",
      "central difference: 0.0022649762686342\n"
     ]
    }
   ],
   "source": [
    "# compute forward loss\n",
    "model.aw1.t.copy_(torch.tensor(origin_t + h))\n",
    "print(model.aw1.t.item())\n",
    "logits = model(x)\n",
    "forward = criterion(logits, y)\n",
    "model.aw1.t.copy_(torch.tensor(origin_t - h))\n",
    "print(model.aw1.t.item())\n",
    "logits = model(x)\n",
    "backward = criterion(logits, y)\n",
    "\n",
    "model.aw1.t.copy_(torch.tensor(origin_t))\n",
    "print(f'base loss: {base.item()}\\nforward loss: {forward.item()}\\nbackward loss: {backward.item()}\\n'\\\n",
    "      f'forward difference: {(forward - base) / h}\\n'\\\n",
    "      f'backward difference: {(base - backward) / h}\\n'\\\n",
    "      f'central difference: {(forward - backward) / (2 * h)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d062d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_numerical(model, func, x, h=1e-4):\n",
    "    logits = func(x)\n",
    "    forward = func(x + h)\n",
    "    backward = func(x - h)\n",
    "\n",
    "    print(f'forward difference {(forward - logits) / h}\\nbackward difference: {(logits - backward) / h}\\ncentral difference: {(forward-backward)/ (2* h)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_t(x, t=1):\n",
    "    return np.exp(x / t) / np.sum(np.exp(x / t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06f756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_pool(x, t):\n",
    "    return x.dot(softmax_t(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a06bfdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05000000074505806 tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(0.05)\n",
    "b = a.item()\n",
    "a.copy_(torch.tensor(3))\n",
    "print(b, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7903e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWPooling",
   "language": "python",
   "name": "swpooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
